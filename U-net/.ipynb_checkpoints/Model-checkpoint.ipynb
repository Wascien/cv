{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb674ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from DataModel import SegmentationData,base_image_path,base_imge_name,base_label_path,base_label_name,base_path\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_channels, middle_channels, out_channels):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "        self.conv_relu = nn.Sequential(\n",
    "            nn.Conv2d(middle_channels, out_channels, kernel_size=3,stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            )\n",
    "  \n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "       \n",
    "        x1 = torch.cat((x1, x2), dim=1)\n",
    "        x1 = self.conv_relu(x1)\n",
    "        return x1\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels,use_maxpool=True):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        layers=[]\n",
    "        \n",
    "        if use_maxpool==True:\n",
    "            layers.append(nn.MaxPool2d(kernel_size=2,stride=2))\n",
    "        \n",
    "        layers+=[\n",
    "            nn.Conv2d(in_channels,out_channels,kernel_size=3,stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels,out_channels,kernel_size=3,stride=1),\n",
    "            nn.ReLU(),\n",
    "        ]\n",
    "        self.net=nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self,X):\n",
    "        \n",
    "        return self.net(X)\n",
    "\n",
    "    \n",
    "encoder_params=((3,64,False),(64,128),(128,256),(256,512),(512,1024))    \n",
    "decoder_params=((1024,1024,512),(512,512,256),(256,256,128),(128,128,64)) \n",
    "class U_net(nn.Module):\n",
    "    def __init__(self,encoder_params,decoder_params):\n",
    "        super().__init__()\n",
    "        maxpools=[]\n",
    "        encoders=[]\n",
    "        for param in encoder_params:\n",
    "            encoders.append(Encoder(*param))\n",
    "            maxpools.append(nn.MaxPool2d(kernel_size=2,stride=2))\n",
    "            \n",
    "        decoders=[]\n",
    "        for param in decoder_params:\n",
    "            decoders.append(Decoder(*param))\n",
    "        \n",
    "        self.encoders=nn.Sequential(*encoders)\n",
    "        self.decoders=nn.Sequential(*decoders)\n",
    "        self.finalconv=nn.Conv2d(64,21,kernel_size=1)\n",
    "       \n",
    "        \n",
    "        \n",
    "    def forward(self,X):\n",
    "\n",
    "        x0=self.encoders[0](X)\n",
    "        #print(x0.shape)\n",
    "        x1=self.encoders[1](x0)\n",
    "        #print(x1.shape)\n",
    "        x2=self.encoders[2](x1)\n",
    "        #print(x2.shape)\n",
    "        x3=self.encoders[3](x2)\n",
    "        #print(x3.shape)\n",
    "        x4=self.encoders[4](x3)\n",
    "        #print(x4.shape)\n",
    "        e3=x3[:,:,4:60,4:60]\n",
    "        x5=self.decoders[0](x4,e3)\n",
    "        #print(x5.shape)\n",
    "        e2=x2[:,:,16:120,16:120]\n",
    "        x6=self.decoders[1](x5,e2)\n",
    "        #print(x6.shape)\n",
    "        e1=x1[:,:,40:240,40:240]\n",
    "        x7=self.decoders[2](x6,e1)\n",
    "        e0=x0[:,:,98:490,98:490]\n",
    "        #print(x7.shape)\n",
    "        x8=self.decoders[3](x7,e0)\n",
    "        return self.finalconv(x8)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82b6caf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=SegmentationData(base_path+'train.txt',base_image_path,base_label_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf1ee368",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter=DataLoader(data,batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57e29d43",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "net=U_net(encoder_params,decoder_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e401028d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for m in net.modules():\\n    if isinstance(m, (nn.Conv2d, nn.Linear)):\\n        nn.init.xavier_uniform_(m.weight)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def precise(test_iter,net,device):\n",
    "    total=0\n",
    "    correct=0\n",
    "    with torch.no_grad():\n",
    "        for X,y in test_iter:\n",
    "            X,y=X.to(device),y.to(device)\n",
    "            y_hat=net(X)\n",
    "            y_hat=torch.argmax(y_hat,dim=-1).flatten()\n",
    "            y=y.flatten()\n",
    "            ans=(y_hat==y)\n",
    "            total+=len(y)\n",
    "            correct+=ans.sum().item()\n",
    "        \n",
    "    print(correct)\n",
    "    print(f\"accuracy :{correct/total*100:>3f}% \")\n",
    "\n",
    "    \n",
    "def train(data_iter,entroy_iter,net,optimizer,lr_scheduler,loss_fn,epochs,device,epoch_data_num):\n",
    "    matrix_x,matrix_loss,entroy_loss,entroy_x=[0],[0],[],[]\n",
    "    total_loss=0\n",
    "    batchs=len(data_iter)\n",
    "    for epoch in range(epochs):\n",
    "        now_num=0\n",
    "        for X,y in data_iter:\n",
    " \n",
    "            now_num+=len(X)\n",
    "    \n",
    "            net.train()\n",
    "            \n",
    "            X,y=X.to(device),y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_hat=net(X)\n",
    "            y_hat=y_hat.permute(0,2,3,1).flatten(start_dim=0,end_dim=-2)\n",
    "            y=y.flatten()\n",
    "            loss=loss_fn(y_hat,y)\n",
    "            loss.sum().backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss+=loss.item()\n",
    "\n",
    "            matrix_x.append(matrix_x[-1]+1)\n",
    "            matrix_loss.append(total_loss/(epoch*epoch_data_num+now_num))\n",
    "            \n",
    "            print(f\"loss: {matrix_loss[-1]:>7f} now {matrix_x[-1]}/{batchs*epochs}\",end='\\r')\n",
    "        '''\n",
    "            lr_scheduler.step()\n",
    "            with torch.no_grad():\n",
    "                c_total_loss=0\n",
    "                test_data_num=0\n",
    "                for X,y in entroy_iter: \n",
    "                    net.eval()\n",
    "                    test_data_num+=len(X)\n",
    "                    X,y=X.to(device),y.to(device)\n",
    "                    y_hat=net(X)\n",
    "                    loss=loss_fn(y_hat,y)\n",
    "\n",
    "                    c_total_loss+=loss.item()\n",
    "\n",
    "                print(test_data_num)\n",
    "                entroy_loss.append(c_total_loss/test_data_num)\n",
    "                entroy_x.append((epoch+1)*batchs)\n",
    "            print(f\"cross entroy loss:{entroy_loss[-1]} now {epoch+1}/{epochs}\")\n",
    "            torch.save(net.state_dict(), f\"Google_epoch{epoch+6}.bin\")\n",
    "            precise(test_iter,net,device)\"\"\n",
    "\n",
    "          '''\n",
    "    \n",
    "    return net,matrix_x,matrix_loss,entroy_x,entroy_loss\n",
    "\n",
    "\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "net=U_net(encoder_params,decoder_params).to(device)\n",
    "optimizer=Adam(net.parameters(),lr=0.0016)\n",
    "lr_scheduler=LambdaLR(optimizer, lr_lambda=lambda epoch: 1/(2**epoch))\n",
    "loss_fn=nn.CrossEntropyLoss()\n",
    "'''for m in net.modules():\n",
    "    if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d4b785b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.364065 now 532/5856\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m _,matrix_x,matrix_loss,entroy_x,entroy_loss\u001b[38;5;241m=\u001b[39m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(data_iter, entroy_iter, net, optimizer, lr_scheduler, loss_fn, epochs, device, epoch_data_num)\u001b[0m\n\u001b[0;32m     35\u001b[0m loss\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     36\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 38\u001b[0m total_loss\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m matrix_x\u001b[38;5;241m.\u001b[39mappend(matrix_x[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     41\u001b[0m matrix_loss\u001b[38;5;241m.\u001b[39mappend(total_loss\u001b[38;5;241m/\u001b[39m(epoch\u001b[38;5;241m*\u001b[39mepoch_data_num\u001b[38;5;241m+\u001b[39mnow_num))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "_,matrix_x,matrix_loss,entroy_x,entroy_loss=train(train_iter,train_iter,net,optimizer,lr_scheduler,loss_fn,4,device,len(data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15049603",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
